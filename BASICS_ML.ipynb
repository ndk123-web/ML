{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a377550679cdf1d1",
   "metadata": {},
   "source": [
    "# Scikit Learn Tutorial\n",
    "    - We have Data (Split into two parts)\n",
    "        - X (Features / Inputs)\n",
    "        - Y (Labels / Outputs)\n",
    "    - Then we give \"X\" and \"Y\" to ML mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "dba395100026f861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.064001Z",
     "start_time": "2025-02-09T12:19:49.054900Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import index\n",
    "from unittest.mock import inplace\n",
    "\n",
    "from sklearn import datasets\n",
    "iris_dataset = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4f43c8f628e10",
   "metadata": {},
   "source": [
    "- iris_data sends a dictionary containing fields\n",
    "    ['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module']\n",
    "- X = iris_dataset['data']   -> Input fields (features)\n",
    "- y = iris_dataset['target'] -> Output fields (Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d2212f0ebbe1cc3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.075330Z",
     "start_time": "2025-02-09T12:19:49.064001Z"
    }
   },
   "outputs": [],
   "source": [
    "X = iris_dataset['data']\n",
    "y = iris_dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba6ff7e99f7e34",
   "metadata": {},
   "source": [
    "### Now we need to give this data to ML modes (Linear Regression)\n",
    "    - ML models are simply Python Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ad1a1e1cffb10e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.107909Z",
     "start_time": "2025-02-09T12:19:49.099613Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "81e9616b4f89157e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.156698Z",
     "start_time": "2025-02-09T12:19:49.139514Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model1 = KNeighborsClassifier()\n",
    "model1.fit(X,y)\n",
    "model1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2c9ebd567f884394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.285176Z",
     "start_time": "2025-02-09T12:19:49.183580Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_pred = model1.predict(X)\n",
    "plt.scatter(y_pred,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98755de194f569c",
   "metadata": {},
   "source": [
    "## Data cleaning and Data Preprocessing\n",
    "    - To maintain Data Constsistency\n",
    "    - So ML models will not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cf5c1126eb1fbe68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.331367Z",
     "start_time": "2025-02-09T12:19:49.311810Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "'''\n",
    "    fetch_openml -> return dictionary objects\n",
    "    as_frame = True -> means return as a Dataframe Object\n",
    "'''\n",
    "\n",
    "df = fetch_openml('titanic',version=1,as_frame=True)\n",
    "x = df['data']\n",
    "y = df['target']\n",
    "\n",
    "checking_null = x.isnull().sum()\n",
    "checking_null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3ae4178f7f67d",
   "metadata": {},
   "source": [
    "### Data Visualization of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "84c4e879a3f07504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.526066Z",
     "start_time": "2025-02-09T12:19:49.386474Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style='dark')\n",
    "# checking_null.plot(kind='bar',title='Null Values',ylabel='Percentage')\n",
    "\n",
    "checking_null = checking_null.to_frame(name=\"Nulls\")\n",
    "checking_null['Names'] = checking_null.index\n",
    "sns.barplot(x=\"Nulls\",y='Names',data=checking_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9150a216b40d5",
   "metadata": {},
   "source": [
    "### As u can see Body has maximum Nulls\n",
    "    - So we might need to remove 'body' column\n",
    "    -- but it's not optimized approach , by removing we r removing factor that may contribute to our outcome\n",
    "\n",
    "-- So For that we need to use Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3c7ff9a89f3fdbbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.569422Z",
     "start_time": "2025-02-09T12:19:49.560665Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([x,y],axis=1) # sklearn bunch object to dataframe\n",
    "print(\"Shape Before Remove\",df.shape)\n",
    "df.drop(['body'] , axis = 1 , inplace=True)\n",
    "print('Shape After Remove',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48489bd797b43e",
   "metadata": {},
   "source": [
    "### So we will Use \" Value Imputation \"\n",
    "    - We will use SimpleImputer from sklearn\n",
    "    - It will replace missing values with some statistics calculated from other values in a column\n",
    "    - Used : Mean , Median , Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ced373e45a949e15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.595502Z",
     "start_time": "2025-02-09T12:19:49.586998Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "print(\"Number of Null Values in Age Before : \",df['age'].isnull().sum())\n",
    "\n",
    "# It means jaha null dikhha waha pe mean place kr dega\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "\n",
    "# imputer takes Dataframe or 2d array\n",
    "# and in place of null values it will place mean\n",
    "df['age'] = imp.fit_transform(df[['age']])\n",
    "\n",
    "print(\"Number of Null Values in Age After : \",df['age'].isnull().sum())\n",
    "print('Means is : ',np.mean(df['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "cd6ed3beba4e1ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.626445Z",
     "start_time": "2025-02-09T12:19:49.616878Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_parameters(df):\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype in ['int64','float64']:\n",
    "            strategy = 'mean'\n",
    "        else:\n",
    "            strategy = 'most_frequent'\n",
    "\n",
    "        missing_values = 'Nan' if df[column].isnull().sum() > 0 else 'None'\n",
    "        parameters[column] = {'missing_values': int(df[column].isnull().sum()) , 'strategy':strategy}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "parameters = get_parameters(df)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "726def6755ce0374",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.676782Z",
     "start_time": "2025-02-09T12:19:49.646375Z"
    }
   },
   "outputs": [],
   "source": [
    "for column , parameter in parameters.items():\n",
    "    check_null_sums = parameter['missing_values']\n",
    "    imp = SimpleImputer(strategy=parameter['strategy'])\n",
    "    df[column] = imp.fit_transform(df[[column]]).ravel()\n",
    "\n",
    "df.isnull().sum()\n",
    "# print(df.isnull().sum()) # check imputed or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed098163263ec8",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "    ðŸš€ Feature Engineering Techniques\n",
    "    âœ… Missing Value Handling (Mean, Median, Mode)\n",
    "    âœ… Categorical Encoding (One-Hot, Label Encoding)\n",
    "    âœ… Feature Scaling (Normalization, Standardization)\n",
    "    âœ… Feature Creation (New features like family_size, is_alone)\n",
    "    âœ… Feature Selection (Remove irrelevant or correlated features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "978ed0f09ba5752a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.732447Z",
     "start_time": "2025-02-09T12:19:49.721509Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bd7f00c657b3c7ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:19:49.871857Z",
     "start_time": "2025-02-09T12:19:49.784651Z"
    }
   },
   "outputs": [],
   "source": [
    "df['family'] = df['sibsp'] + df['parch']\n",
    "\n",
    "df.loc[df['family'] > 0 , 'travel_alone'] = 0\n",
    "df.loc[df['family'] == 0 , 'travel_alone'] = 1\n",
    "sns.set()\n",
    "df['travel_alone'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48132ad3d9577ad",
   "metadata": {},
   "source": [
    "### Data Encoding\n",
    "    - Its Simply Converts Categorical (string) into number like format\n",
    "    - so ML model understands\n",
    "    - Ex : SEX - [male,female] -> for ML we converts = SEX - [1,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "617cae932a81ad4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:20:18.975766Z",
     "start_time": "2025-02-09T12:20:18.964771Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encode = OneHotEncoder()\n",
    "\n",
    "# creates new columns female and male and ,\n",
    "# uses fit_transform() and return array so the each array will place at ,\n",
    "# specific column\n",
    "# Ex : [ [1,0].[0,1].[0,1].[1,0] ] then\n",
    "# female -> arr[1,0,0,1] and male -> arr[0,1,1,0]\n",
    "\n",
    "# toarrray returns 2d numpy array\n",
    "# 2d numpy array automatically place column 0 as female and column 1 as male\n",
    "# fit_transform usually encode categorical value into numeric on basis of sex columm\n",
    "df[['female','male']] = encode.fit_transform(df[['sex']]).toarray()\n",
    "df[['sex','female','male']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "35ca12175a786336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T12:40:01.994764Z",
     "start_time": "2025-02-09T12:40:01.978140Z"
    }
   },
   "outputs": [],
   "source": [
    "# returns 2d numpy array\n",
    "arr = encode.fit_transform(df[['sex']]).toarray()\n",
    "\n",
    "# all values from column 0 to sex column\n",
    "df['sex'] = arr[:,0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6e4994a4d831a",
   "metadata": {},
   "source": [
    "### Data Scaling\n",
    "    - if the data in any condition has data points far from each other,\n",
    "    - Scaling is a technique to make them closer to each other\n",
    "    - Data scaling is done so that no feature dominates the model just because of its larger\n",
    "      numerical values.\n",
    "    - StandardScaler -> Standardize features by removing the mean and scaling to unit variance\n",
    "    - MinMaxScaler -> Scale features to a given range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "dada8b04e08b685a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T13:10:04.351635Z",
     "start_time": "2025-02-09T13:10:04.306002Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "'''\n",
    "    StandardScaler -> Standardizes features by removing the mean and scaling to unit variance.\n",
    "    Formula: X' = (X - mean) / standard deviation\n",
    "    - Centers the data around mean = 0\n",
    "    - Scales features so that standard deviation = 1\n",
    "    - Useful when data follows a normal distribution.\n",
    "'''\n",
    "\n",
    "# Select numerical columns (returns a Pandas Index object, which is immutable)\n",
    "nums_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(nums_cols)  # List of numerical column names\n",
    "\n",
    "# Initialize StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# Apply StandardScaler to selected numerical columns\n",
    "df[nums_cols] = std_scaler.fit_transform(df[nums_cols])\n",
    "\n",
    "# Display first 5 rows after Standard Scaling\n",
    "df[nums_cols].head()\n",
    "\n",
    "'''\n",
    "    MinMaxScaler -> Scales features to a fixed range (default: 0 to 1)\n",
    "    Formula: X' = (X - X_min) / (X_max - X_min)\n",
    "    - Ensures all features are within a specified range (default: 0 to 1)\n",
    "    - Useful when data does not follow a normal distribution.\n",
    "'''\n",
    "\n",
    "# Select numerical columns again (to apply MinMax scaling)\n",
    "nums_cols1 = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "mnmx_scaler = MinMaxScaler()\n",
    "\n",
    "# Apply MinMaxScaler to selected numerical columns\n",
    "df[nums_cols1] = mnmx_scaler.fit_transform(df[nums_cols1])\n",
    "\n",
    "# Display transformed data\n",
    "df[nums_cols1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
